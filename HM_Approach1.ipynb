{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94564f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import ast\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_transaction = pd.read_csv('path/transactions_train.csv')\n",
    "hm_transaction['article_id'] = hm_transaction.article_id.astype(str)\n",
    "hm_trsc = hm_transaction.drop(['price', 'sales_channel_id'], axis=1)\n",
    "hm_trsc['t_dat'] = pd.to_datetime(hm_trsc['t_dat'])\n",
    "\n",
    "hm_article = pd.read_csv('path/articles.csv')\n",
    "hm_arti = hm_article[['article_id', 'prod_name']]#, 'product_type_name']]\n",
    "hm_arti['article_id'] = hm_arti.article_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_test = hm_trsc[hm_trsc.t_dat >='2020-09-01']\n",
    "hm_val = hm_trsc[(hm_trsc.t_dat < '2020-09-01') & (hm_trsc.t_dat >= '2020-06-01')]\n",
    "hm_train = hm_trsc[hm_trsc.t_dat < '2020-06-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = pd.read_csv('path/img_feat_vgg_cosine.csv')\n",
    "sim = sim.drop(['Unnamed: 0','top_dist'],axis=1)\n",
    "sim = sim.drop_duplicates('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del hm_transaction, hm_article\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72021dfa",
   "metadata": {},
   "source": [
    "# Prediction without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate Generation from hm_val set\n",
    "hm_full = hm_test[['customer_id']]\n",
    "hm_full = pd.merge(hm_full, hm_val, how='customer_id', how='left')\n",
    "hm_full = hm_full[['t_dat','customer_id','article_id']]\n",
    "hm_full = pd.merge(hm_full, sim, on='article_id',how='left')\n",
    "hm_full = pd.merge(hm_full, hm_arti,on='article_id',how='left' )\n",
    "hm_full = hm_full[['customer_id','top_sim','prod_name']].rename(columns=\n",
    "                                                {'top_sim':'article_id'}).dropna(subset='article_id')\n",
    "# lastest 55 transactions for each customer\n",
    "trsc_train = hm_full.copy()\n",
    "trsc_train = trsc_train.groupby('customer_id').tail(55)\n",
    "\n",
    "count_prod_name = hm_full.groupby(['prod_name','article_id'])['customer_id'].agg('count').reset_index().sort_values(['customer_id'], ascending=False)\n",
    "top5_prod_ppl = count_prod_name.groupby('prod_name')['article_id'].apply(' '.join).reset_index()\n",
    "for i in range(len(top5_prod_ppl)):\n",
    "    top5_prod_ppl.article_id[i] = list(top5_prod_ppl.article_id[i].split())[:30]\n",
    "prod_train = hm_full.copy()\n",
    "prod_train = prod_train.drop_duplicates(['customer_id'], keep='last').drop('article_id', axis=1)\n",
    "prod_train = pd.merge(prod_train, top5_prod_ppl, on='prod_name', how='left')\n",
    "prod_train = prod_train.explode('article_id')\n",
    "\n",
    "top5_ppl = hm_full.article_id.value_counts().to_frame().index.astype('str')[:85] #for cold-start customers\n",
    "top5_ppl_train = hm_full.copy()\n",
    "top5_ppl_train = top5_ppl_train.drop_duplicates(['customer_id'], keep='last').drop('article_id', axis=1)\n",
    "top5_ppl_train['article_id'] = [list(top5_ppl) for _ in range(len(top5_ppl_train))]\n",
    "top5_ppl_train = top5_ppl_train.explode('article_id')\n",
    "\n",
    "hm_pred = pd.concat([trsc_train, prod_train, top5_ppl_train])\n",
    "hm_pred = hm_pred.groupby('customer_id').head(85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86463127",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (\n",
    "    hm_pred\n",
    "    .groupby('customer_id')[['article_id']]\n",
    "    .aggregate(lambda x: x.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = hm_test.drop(['index','t_dat'],axis=1)\n",
    "test = test.groupby('customer_id')[['article_id']].aggregate(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(preds.article_id)\n",
    "true = list(test.article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a710d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(true_labels, pred_labels, k):\n",
    "    true_positives = 0\n",
    "    total_positives = len(true_labels)\n",
    "    if total_positives > k:\n",
    "        true_labels = true_labels[:k]\n",
    "    for pred in pred_labels[:k]:\n",
    "        if pred in true_labels:\n",
    "            true_positives += 1\n",
    "    recall = true_positives / total_positives if total_positives > 0 else 0\n",
    "    return recall\n",
    "\n",
    "def calculate_recall_at_k(true_labels_list, pred_labels_list, k):\n",
    "    recall_scores = []\n",
    "    for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
    "        recall = recall_at_k(true_labels, pred_labels, k)\n",
    "        recall_scores.append(recall)\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "    return mean_recall\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asarray(r)[:k]\n",
    "    discounts = np.log2(np.arange(len(r)) + 2)\n",
    "    return np.sum(r / discounts)\n",
    "\n",
    "def ndcg_at_k(true_labels, pred_scores, k):\n",
    "    dcg = dcg_at_k(true_labels, k)\n",
    "    idcg = dcg_at_k(sorted(true_labels, reverse=True), k)\n",
    "    return dcg / idcg if idcg != 0 else 0\n",
    "\n",
    "def calculate_ndcg_at_k(y_true, y_pred, k):\n",
    "    ndcg_scores = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        relevance = [1 if item in true_items else 0 for item in pred_items]\n",
    "        ndcg_scores.append(ndcg_at_k(relevance, relevance,k))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def average_precision_at_k(true_labels, pred_labels, k):\n",
    "    num_correct = 0\n",
    "    precision_sum = 0\n",
    "    for i, pred in enumerate(pred_labels[:k]):\n",
    "        if pred in true_labels:\n",
    "            num_correct += 1\n",
    "            precision_sum += num_correct / (i + 1)\n",
    "    return precision_sum / min(k, len(true_labels))\n",
    "\n",
    "def map_at_k(true_labels_list, pred_labels_list, k):\n",
    "    average_precisions = []\n",
    "    for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
    "        average_precision = average_precision_at_k(true_labels, pred_labels, k)\n",
    "        average_precisions.append(average_precision)\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(calculate_recall_at_k(true, prediction, 5),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 10),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(map_at_k(true, prediction, 5),5))\n",
    "print(round(map_at_k(true, prediction, 10),5))\n",
    "print(round(map_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(calculate_ndcg_at_k(true, prediction,5),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,10),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,20),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac9be5",
   "metadata": {},
   "source": [
    "# Prediction with Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35814f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_dict_4w = {}\n",
    "hm_4w = hm_val\n",
    "for i,x in enumerate(zip(hm_4w['customer_id'], hm_4w['article_id'])):\n",
    "    cust_id, art_id = x\n",
    "    if cust_id not in purchase_dict_4w:\n",
    "        purchase_dict_4w[cust_id] = {}\n",
    "    \n",
    "    if art_id not in purchase_dict_4w[cust_id]:\n",
    "        purchase_dict_4w[cust_id][art_id] = 0\n",
    "    \n",
    "    purchase_dict_4w[cust_id][art_id] += 1\n",
    "\n",
    "dummy_list_4w = list((hm_4w['article_id'].value_counts()).index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_candidates(customers_id, n_candidates = 10):\n",
    "  \"\"\"\n",
    "  df - basically, dataframe with customers(customers should be unique)\n",
    "  \"\"\"\n",
    "  prediction_dict = {}\n",
    "  dummy_list = list((hm_4w['article_id'].value_counts()).index)[:n_candidates]\n",
    "\n",
    "  for i, cust_id in enumerate(customers_id):\n",
    "    # comment this for validation\n",
    "    if cust_id in purchase_dict_4w:\n",
    "        l = sorted((purchase_dict_4w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "        l = [y[0] for y in l]\n",
    "        if len(l)>n_candidates:\n",
    "            s = l[:n_candidates]\n",
    "        else:\n",
    "            s = l+dummy_list_4w[:(n_candidates-len(l))]\n",
    "    else:\n",
    "        s = dummy_list\n",
    "    prediction_dict[cust_id] = s\n",
    "\n",
    "  k = list(map(lambda x: x[0], prediction_dict.items()))\n",
    "  v = list(map(lambda x: x[1], prediction_dict.items()))\n",
    "  negatives_df = pd.DataFrame({'customer_id': k, 'negatives': v})\n",
    "  negatives_df = (\n",
    "      negatives_df\n",
    "      .explode('negatives')\n",
    "      .rename(columns = {'negatives': 'article_id'})\n",
    "  )\n",
    "  return negatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_pickle('hm_embeddings_resnet.pkl')\n",
    "img_id = pd.read_pickle('filenames.pkl')\n",
    "img_feat = pd.DataFrame(img_id, columns=['article_id'])\n",
    "img_feat['article_id'] = [img_feat.article_id[i][1:] for i in range(len(img_feat.article_id))]\n",
    "img_feat['img_embedding'] = embeddings\n",
    "img_feat = img_feat.drop_duplicates('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf89826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = hm_val.drop_duplicates(subset=['customer_id','article_id'])\n",
    "pos = pd.merge(hm_train, sim, on='article_id', how='left')\n",
    "pos = pd.merge(pos, df_val,left_on=['customer_id','top_sim'], right_on=['customer_id', 'article_id'], how='inner')\n",
    "pos = pos.drop(['t_dat_x','t_dat_y', 'article_id_x','article_id_y'],axis=1).rename(columns={'top_sim':'article_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7dd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos.dropna(subset='article_id')\n",
    "customer_id = pos['customer_id'].unique()\n",
    "positives = prepare_candidates(customer_id, 50)\n",
    "positives = positives.reset_index(drop=True).set_index(['customer_id','article_id'])\n",
    "positives['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b159a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_train = pd.merge(hm_train, sim, on='article_id',how='left')\n",
    "neg = sim_train[['customer_id','top_sim']].reset_index(drop=True).set_index(['customer_id','top_sim'])\n",
    "negatives = neg[~neg.index.isin(list(set(positives.index)))].reset_index()\n",
    "positives = positives.reset_index()\n",
    "negatives['label'] = 0\n",
    "negatives = negatives.rename(columns={'top_sim':'article_id'})\n",
    "negatives = negatives[~negatives.article_id.isna()]\n",
    "negatives = negatives[negatives.customer_id.isin(list(set(positives.customer_id)))]\n",
    "negatives = negatives.groupby('customer_id').tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_full = pd.concat([positives, negatives],ignore_index=True)\n",
    "sia_full = sia_full.drop_duplicates(['customer_id','article_id'])\n",
    "sia_full = pd.merge(sia_full, img_feat, on='article_id',how='left')\n",
    "sia_full = sia_full[~sia_full.img_embedding.isna()]\n",
    "sia_full['label'] = sia_full.label.astype(int)\n",
    "sia_full = sia_full.sample(frac=1)\n",
    "sia_train, sia_val = np.split(sia_full, [int(0.8*len(sia_full))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(data):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    cust_ids = data.customer_id.unique()\n",
    "    \n",
    "    for j in tqdm.tqdm(cust_ids):\n",
    "        sets = data[data.customer_id == j].reset_index(drop=True)\n",
    "            \n",
    "        x = sets.img_embedding.reset_index(drop=True)\n",
    "        y = sets.label.reset_index(drop=True)\n",
    "        \n",
    "        num_classes = max(y) + 1\n",
    "        digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
    "\n",
    "        for idx1 in range(len(x)):\n",
    "            # measure cross-group similarity, so if the items belong to the same category, it's consideres as dissimilar\n",
    "            # add a matching example\n",
    "            x1 = x[idx1]\n",
    "            label1 = y[idx1]\n",
    "\n",
    "            if len(digit_indices[label1]) == 1:\n",
    "                continue\n",
    "            idx2 = random.choice(digit_indices[label1])\n",
    "\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [0]\n",
    "            \n",
    "            if len(digit_indices) == 1:\n",
    "                continue\n",
    "            # add a non-matching example\n",
    "            label2 = 0 if label1 == 1 else 1\n",
    "            if len(digit_indices[label2]) == 0:\n",
    "                continue\n",
    "            idx2 = random.choice(digit_indices[label2])\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [1]\n",
    "            #print(j)\n",
    "\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, labels_train = make_pairs(sia_train)\n",
    "pairs_val, labels_val = make_pairs(sia_val)\n",
    "\n",
    "train_1 = pairs_train[:, 0] \n",
    "train_2 = pairs_train[:, 1]\n",
    "val_1 = pairs_val[:, 0]\n",
    "val_2 = pairs_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "input_1 = layers.Input(2048)\n",
    "input_2 = layers.Input(2048)\n",
    "merge_layer = layers.Lambda(euclidean_distance)([input_1, input_2])\n",
    "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "\n",
    "def loss(margin=1):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing constrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss\n",
    "\n",
    "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05114bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese.fit(\n",
    "    [train_1, train_2],\n",
    "    labels_train,\n",
    "    validation_data=([val_1, val_2], labels_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8694cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metric(history, metric, title, has_valid=True):\n",
    "    plt.plot(history[metric])\n",
    "    if has_valid:\n",
    "        plt.plot(history[\"val_\" + metric])\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the accuracy\n",
    "plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = hm_pred[['customer_id']].drop_duplicates(subset='customer_id')\n",
    "purchased = hm_val.groupby(['customer_id','article_id']).t_dat.count().reset_index().sort_values(\n",
    "                                                                't_dat',ascending=False)\n",
    "purchased = purchased.groupby(['customer_id']).head(5)\n",
    "purchased = purchased[purchased.customer_id.isin(list(group_a.customer_id))]\n",
    "group_a = pd.merge(group_a,purchased,on='customer_id',how='left')\n",
    "\n",
    "# find most purchased item for cold_start customers\n",
    "ppl = hm_val.article_id.value_counts().to_frame().index.astype('str')[1:5]\n",
    "ppl_df = hm_pred[['customer_id']].drop_duplicates(subset='customer_id')\n",
    "ppl_df['article_id'] =[ list(ppl) for _ in range(len(ppl_df))]\n",
    "ppl_df = ppl_df.explode('article_id')\n",
    "\n",
    "group_a = pd.concat([group_a,ppl_df],ignore_index=True)\n",
    "group_a = group_a[group_a.article_id.isin(list(img_feat.article_id))]\n",
    "\n",
    "group_a = group_a.merge(img_feat, on='article_id',how='left')\n",
    "hm_pred = hm_pred.merge(img_feat,on='article_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_pairs(data,group_b):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    cust_ids = data.customer_id.unique()\n",
    "    \n",
    "    for j in tqdm.tqdm(cust_ids):\n",
    "        sets_a = data[data.customer_id == j].reset_index(drop=True)\n",
    "        sets_b = group_b[group_b.customer_id == j].reset_index(drop=True)\n",
    "            \n",
    "        x_a = sets_a.img_embedding\n",
    "        x_b = sets_b.img_embedding\n",
    "        #y = sets.label\n",
    "\n",
    "        #digit_indices = np.where(y == 1)[0] \n",
    "\n",
    "        for idx1 in range(len(x_a)):\n",
    "            for idx2 in range(len(x_b)):\n",
    "                # measure cross-group similarity, so if the items belong to the same category, it's consideres as dissimilar\n",
    "                # add a matching example\n",
    "                x1 = x_a[idx1]\n",
    "                x2 = x_b[idx2]\n",
    "\n",
    "                pairs += [[x1, x2]]\n",
    "                labels += [0]\n",
    "\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_test, labels_test = make_test_pairs(group_a, hm_pred)\n",
    "test_1 = pairs_test[:, 0]\n",
    "test_2 = pairs_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae008a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 5000\n",
    "for bucket in tqdm.tqdm(range(0, len(test_1), batch_size)):\n",
    "  outputs = siamese.predict(\n",
    "      [test_1[bucket: bucket+batch_size], test_2[bucket: bucket+batch_size] ]\n",
    "      )\n",
    "  predictions.append(outputs)\n",
    "predictions = np.concatenate(predictions)\n",
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4946a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred = hm_pred.copy()\n",
    "sia_pred['pred'] = predictions\n",
    "sia_pred = sia_pred.sort_values(['customer_id','pred'],ascending=False)\n",
    "sia_pred = sia_pred.groupby(['customer_id','article_id']).prod_name.agg('count').reset_index().sort_values('prod_name',ascending=False)\n",
    "sia_pred = sia_pred.groupby('customer_id').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred_lst = (\n",
    "    sia_pred\n",
    "    .groupby('customer_id')[['article_id']]\n",
    "    .aggregate(lambda x: x.tolist())\n",
    ")\n",
    "prediction = list(sia_pred_lst.article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b25cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(calculate_recall_at_k(true, prediction, 5),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 10),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(map_at_k(true, prediction, 5),5))\n",
    "print(round(map_at_k(true, prediction, 10),5))\n",
    "print(round(map_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(calculate_ndcg_at_k(true, prediction,5),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,10),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,20),5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
