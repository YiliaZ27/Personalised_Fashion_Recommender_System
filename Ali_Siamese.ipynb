{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fcecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cfbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(data):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    cust_ids = data.user_id.unique()\n",
    "    \n",
    "    for j in tqdm.tqdm(cust_ids):\n",
    "        sets = data[data.user_id == j].reset_index(drop=True)\n",
    "            \n",
    "        x = sets.img_embedding.reset_index(drop=True)\n",
    "        y = sets.label.reset_index(drop=True)\n",
    "        \n",
    "        num_classes = max(y) + 1\n",
    "        digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
    "\n",
    "        for idx1 in range(len(x)):\n",
    "            # add a matching example\n",
    "            x1 = x[idx1]\n",
    "            label1 = y[idx1]\n",
    "\n",
    "            if len(digit_indices[label1]) == 1:\n",
    "                continue\n",
    "            idx2 = random.choice(digit_indices[label1])\n",
    "\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [0]\n",
    "            \n",
    "            if len(digit_indices) == 1:\n",
    "                continue\n",
    "            # add a non-matching example\n",
    "            label2 = 0 if label1 == 1 else 1\n",
    "            if len(digit_indices[label2]) == 0:\n",
    "                continue\n",
    "            idx2 = random.choice(digit_indices[label2])\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [1]\n",
    "            #print(j)\n",
    "\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_pairs(data,group_b):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    cust_ids = data.user_id.unique()\n",
    "    \n",
    "    for j in tqdm.tqdm(cust_ids):\n",
    "        sets_a = data[data.user_id == j].reset_index(drop=True)\n",
    "        sets_b = group_b[group_b.user_id == j].reset_index(drop=True)\n",
    "            \n",
    "        x_a = sets_a.img_embedding\n",
    "        x_b = sets_b.img_embedding\n",
    "\n",
    "\n",
    "        for idx1 in range(len(x_a)):\n",
    "            for idx2 in range(len(x_b)):\n",
    "               # add a matching example\n",
    "                x1 = x_a[idx1]\n",
    "                x2 = x_b[idx2]\n",
    "\n",
    "                pairs += [[x1, x2]]\n",
    "                labels += [0]\n",
    "\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "input_1 = layers.Input(2048)\n",
    "input_2 = layers.Input(2048)\n",
    "merge_layer = layers.Lambda(euclidean_distance)([input_1, input_2])\n",
    "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "\n",
    "def loss(margin=1):\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss\n",
    "\n",
    "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_candidates(customers_id, n_candidates = 10):\n",
    "  prediction_dict = {}\n",
    "  dummy_list = list((ali_4w['item_id'].value_counts()).index)[:n_candidates]\n",
    "\n",
    "  for i, cust_id in enumerate(customers_id):\n",
    "    # comment this for validation\n",
    "    if cust_id in purchase_dict_4w:\n",
    "        l = sorted((purchase_dict_4w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "        l = [y[0] for y in l]\n",
    "        if len(l)>n_candidates:\n",
    "            s = l[:n_candidates]\n",
    "        else:\n",
    "            s = l+dummy_list_4w[:(n_candidates-len(l))]\n",
    "    else:\n",
    "        s = dummy_list\n",
    "    prediction_dict[cust_id] = s\n",
    "\n",
    "  k = list(map(lambda x: x[0], prediction_dict.items()))\n",
    "  v = list(map(lambda x: x[1], prediction_dict.items()))\n",
    "  negatives_df = pd.DataFrame({'user_id': k, 'negatives': v})\n",
    "  negatives_df = (\n",
    "      negatives_df\n",
    "      .explode('negatives')\n",
    "      .rename(columns = {'negatives': 'item_id'})\n",
    "  )\n",
    "  return negatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be21908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(true_labels, pred_labels, k):\n",
    "    true_positives = 0\n",
    "    total_positives = len(true_labels)\n",
    "    if total_positives > k:\n",
    "        true_labels = true_labels[:k]\n",
    "    for pred in pred_labels[:k]:\n",
    "        if pred in true_labels:\n",
    "            true_positives += 1\n",
    "    recall = true_positives / total_positives if total_positives > 0 else 0\n",
    "    return recall\n",
    "\n",
    "def calculate_recall_at_k(true_labels_list, pred_labels_list, k):\n",
    "    recall_scores = []\n",
    "    for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
    "        recall = recall_at_k(true_labels, pred_labels, k)\n",
    "        recall_scores.append(recall)\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "    return mean_recall\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asarray(r)[:k]\n",
    "    discounts = np.log2(np.arange(len(r)) + 2)\n",
    "    return np.sum(r / discounts)\n",
    "\n",
    "def ndcg_at_k(true_labels, pred_scores, k):\n",
    "    dcg = dcg_at_k(true_labels, k)\n",
    "    idcg = dcg_at_k(sorted(true_labels, reverse=True), k)\n",
    "    return dcg / idcg if idcg != 0 else 0\n",
    "\n",
    "def calculate_ndcg_at_k(y_true, y_pred, k):\n",
    "    ndcg_scores = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        relevance = [1 if item in true_items else 0 for item in pred_items]\n",
    "        ndcg_scores.append(ndcg_at_k(relevance, relevance,k))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def average_precision_at_k(true_labels, pred_labels, k):\n",
    "    num_correct = 0\n",
    "    precision_sum = 0\n",
    "    for i, pred in enumerate(pred_labels[:k]):\n",
    "        if pred in true_labels:\n",
    "            num_correct += 1\n",
    "            precision_sum += num_correct / (i + 1)\n",
    "    return precision_sum / min(k, len(true_labels))\n",
    "\n",
    "def map_at_k(true_labels_list, pred_labels_list, k):\n",
    "    average_precisions = []\n",
    "    for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
    "        average_precision = average_precision_at_k(true_labels, pred_labels, k)\n",
    "        average_precisions.append(average_precision)\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b30a6a",
   "metadata": {},
   "source": [
    "# Similar-Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93116ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_user = pd.read_pickle('ali_user.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_pickle('path/ali_embeddings.pkl')\n",
    "img_id = pd.read_pickle('path/filenames.pkl')\n",
    "img_id = img_id[:430000]\n",
    "img_feat = pd.DataFrame(img_id, columns=['item_id'])\n",
    "img_feat['img_embedding'] = embeddings\n",
    "img_feat = img_feat[~img_feat.img_embedding.isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_user = ali_user[ali_user.item_id.isin(list(img_feat.item_id))]\n",
    "outfit = ali_user.groupby('outfit_id').item_id.agg('count').reset_index().sort_values(['item_id'], ascending=False)\n",
    "outfit = outfit[outfit.item_id >=2]\n",
    "ali_user = ali_user[ali_user.outfit_id.isin(list(outfit.outfit_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = pd.read_csv('img_feat_res_euc.csv')\n",
    "sim = sim.drop(['top_dist'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f42ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_user = pd.merge(ali_user, sim, on='item_id',how='left')\n",
    "ali_user = ali_user.sample(frac = 1)\n",
    "ali_train, ali_test, ali_val = np.split(ali_user, [int(0.5*len(ali_user)), int(0.6*len(ali_user))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = ali_val.drop_duplicates(subset=['user_id','item_id'])\n",
    "pos = pd.merge(ali_train, df_val, left_on=['user_id','top_sim'], right_on=['user_id','item_id'],how='inner')\n",
    "pos = pos[['user_id','top_sim_x']].rename(columns={'top_sim_x':'item_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31083d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_dict_4w = {}\n",
    "ali_4w = ali_val\n",
    "for i,x in enumerate(zip(ali_4w['user_id'], ali_4w['item_id'])):\n",
    "    cust_id, art_id = x\n",
    "    if cust_id not in purchase_dict_4w:\n",
    "        purchase_dict_4w[cust_id] = {}\n",
    "    \n",
    "    if art_id not in purchase_dict_4w[cust_id]:\n",
    "        purchase_dict_4w[cust_id][art_id] = 0\n",
    "    \n",
    "    purchase_dict_4w[cust_id][art_id] += 1\n",
    "\n",
    "dummy_list_4w = list((ali_4w['item_id'].value_counts()).index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb810b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = pos['user_id'].unique()\n",
    "positives = prepare_candidates(user_id, 5)\n",
    "positives['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc71016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = positives.drop_duplicates(subset=['user_id','item_id'])\n",
    "lgb_neg = ali_train[ali_train.user_id.isin(list(df2.user_id))]\n",
    "negatives = lgb_neg.merge(df2, left_on =['user_id','top_sim'], right_on=['user_id','item_id'],how='left')\n",
    "negatives = negatives[negatives.label.isna()]\n",
    "negatives['label'] = negatives.label.fillna(0)\n",
    "negatives = negatives[['user_id','top_sim','label']].rename(columns={'top_sim':'item_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_full = pd.concat([positives, negatives],ignore_index=True)\n",
    "sia_full = sia_full.drop_duplicates(['user_id','item_id'])\n",
    "sia_full = pd.merge(sia_full, img_feat, on='item_id',how='left')\n",
    "sia_full['label'] = sia_full.label.astype(int)\n",
    "sia_full = sia_full.sample(frac=1)\n",
    "sia_train, sia_val = np.split(sia_full, [int(0.8*len(sia_full))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, labels_train = make_pairs(sia_train)\n",
    "pairs_val, labels_val = make_pairs(sia_val)\n",
    "train_1 = pairs_train[:, 0] \n",
    "train_2 = pairs_train[:, 1]\n",
    "val_1 = pairs_val[:, 0]\n",
    "val_2 = pairs_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c82032",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese.fit(\n",
    "    [train_1, train_2],\n",
    "    labels_train,\n",
    "    validation_data=([val_1, val_2], labels_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metric(history, metric, title, has_valid=True):\n",
    "    plt.plot(history[metric])\n",
    "    if has_valid:\n",
    "        plt.plot(history[\"val_\" + metric])\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the accuracy\n",
    "plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_full = ali_val.copy()\n",
    "val_full = val_full.drop('item_id',axis=1).rename(columns={'top_sim':'item_id'})\n",
    "top5_ppl = val_full.item_id.value_counts().to_frame().index.astype('str')[:30]\n",
    "\n",
    "top5_ppl_train = ali_test.drop_duplicates(['user_id'], keep='last').drop('item_id', axis=1)\n",
    "top5_ppl_train['item_id'] = [list(top5_ppl) for _ in range(len(top5_ppl_train))]\n",
    "top5_ppl_train = top5_ppl_train.explode('item_id')\n",
    "top5_ppl_train.head()\n",
    "\n",
    "trsc_train = val_full.copy()\n",
    "trsc_train = trsc_train.groupby('user_id').tail(55)\n",
    "trsc_train.head()\n",
    "\n",
    "\n",
    "ali_pred = pd.concat([trsc_train, top5_ppl_train])\n",
    "ali_pred = ali_pred[ali_pred.user_id.isin(list(set(ali_test.user_id)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8970bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = ali_pred[['user_id']].drop_duplicates(subset='user_id')\n",
    "click = ali_val.groupby('user_id').tail(5)\n",
    "group_a = group_a.merge(click, on='user_id',how='left')\n",
    "ppl = ali_val.item_id.value_counts().to_frame().index.astype('str')[0]\n",
    "group_a['item_id'] = group_a['item_id'].fillna(ppl)\n",
    "\n",
    "group_a = group_a.merge(img_feat, on='item_id',how='left')\n",
    "ali_pred = ali_pred.merge(img_feat,on='item_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_test, labels_test = make_test_pairs(group_a, ali_pred)\n",
    "test_1 = pairs_test[:, 0]\n",
    "test_2 = pairs_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 5000\n",
    "for bucket in tqdm.tqdm(range(0, len(test_1), batch_size)):\n",
    "  outputs = siamese.predict(\n",
    "      [test_1[bucket: bucket+batch_size], test_2[bucket: bucket+batch_size] ]\n",
    "      )\n",
    "  predictions.append(outputs)\n",
    "predictions = np.concatenate(predictions)\n",
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ali_test.groupby('user_id')[['item_id']].aggregate(lambda x: x.tolist())\n",
    "sia_pred = ali_pred.copy()\n",
    "sia_pred['pred'] = predictions\n",
    "sia_pred = sia_pred.sort_values(['user_id','pred'],ascending=False)\n",
    "sia_pred = sia_pred.groupby(['user_id','item_id']).img_embedding.agg('count').reset_index().sort_values('img_embedding',ascending=False)\n",
    "sia_pred = sia_pred.groupby('user_id').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred_lst = (\n",
    "    sia_pred\n",
    "    .groupby('user_id')[['item_id']]\n",
    "    .aggregate(lambda x: x.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(sia_pred_lst.item_id)\n",
    "true = list(test.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(calculate_recall_at_k(true, prediction, 5),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 10),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(map_at_k(true, prediction, 5),5))\n",
    "print(round(map_at_k(true, prediction, 10),5))\n",
    "print(round(map_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(calculate_ndcg_at_k(true, prediction,5),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,10),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,20),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a08287",
   "metadata": {},
   "source": [
    "# Compatible-Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_user = pd.read_pickle('ali_user.pkl')\n",
    "ali_user = ali_user[ali_user.item_id.isin(list(img_feat.item_id))]\n",
    "outfit = ali_user.groupby('outfit_id').item_id.agg('count').reset_index().sort_values(['item_id'], ascending=False)\n",
    "outfit = outfit[outfit.item_id >=2]\n",
    "ali_user = ali_user[ali_user.outfit_id.isin(list(outfit.outfit_id))]\n",
    "ali_user = ali_user.sample(frac = 1) # shuffle rows\n",
    "ali_train, ali_test, ali_val = np.split(ali_user, [int(0.5*len(ali_user)), int(0.6*len(ali_user))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = ali_val.drop_duplicates(subset=['user_id','outfit_id','item_id',])\n",
    "identical = pd.merge(ali_train.reset_index(), df_val, on=['user_id','outfit_id','item_id',],how='inner').set_index('index')\n",
    "df_val = ali_val.drop_duplicates(subset=['user_id','outfit_id'])\n",
    "pos = pd.merge(ali_train.reset_index(), df_val, on=['user_id','outfit_id'],how='inner').set_index('index')\n",
    "pos = pos[~pos.index.isin(list(identical.index))].reset_index(drop=True)\n",
    "pos = pos.rename(columns={'item_id_x':'item_id'}).drop('item_id_y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923962d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = ali_train.reset_index(drop=True).set_index(['user_id','item_id'])\n",
    "pos = pos.reset_index(drop=True).set_index(['user_id','item_id'])\n",
    "neg = neg[~neg.index.isin(list(pos.index))].reset_index()\n",
    "pos = pos.reset_index()\n",
    "neg = neg[neg.user_id.isin(list(set(pos.user_id)))]\n",
    "neg = neg.groupby('user_id').filter(lambda x: len(x)>=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos.groupby('user_id').filter(lambda x: len(x)>=5)\n",
    "positives = pos.groupby('user_id').head(10)\n",
    "positives['label'] = 1\n",
    "negatives = neg.groupby('user_id').head(10)\n",
    "negatives['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ac733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_full = pd.concat([positives, negatives],ignore_index=True)\n",
    "sia_full = sia_full.drop_duplicates(['user_id','item_id'])\n",
    "sia_full = pd.merge(sia_full, img_feat, on='item_id',how='left')\n",
    "sia_full = sia_full.sample(frac=1)\n",
    "sia_train, sia_val = np.split(sia_full, [int(0.8*len(sia_full))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, labels_train = make_pairs(sia_train)\n",
    "pairs_val, labels_val = make_pairs(sia_val)\n",
    "train_1 = pairs_train[:, 0] \n",
    "train_2 = pairs_train[:, 1]\n",
    "val_1 = pairs_val[:, 0]\n",
    "val_2 = pairs_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese.fit(\n",
    "    [train_1, train_2],\n",
    "    labels_train,\n",
    "    validation_data=([val_1, val_2], labels_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = ali_test[['user_id']].drop_duplicates(subset='user_id')\n",
    "click = ali_val.groupby('user_id').head(5) #1 for plots purpose\n",
    "group_a = group_a.merge(click, on='user_id',how='left')\n",
    "ppl = ali_train.item_id.value_counts().to_frame().index.astype('str')[0]\n",
    "group_a['item_id'] = group_a['item_id'].fillna(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96dd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outfit = ali_val.drop_duplicates(['user_id','outfit_id','item_id'])\n",
    "ali_pred = ali_test[['user_id']].drop_duplicates(subset='user_id')\n",
    "ali_pred = pd.merge(ali_pred, pred_outfit, on='user_id',how='left')\n",
    "\n",
    "outfit_full = ali_user.drop_duplicates(subset='outfit_id')\n",
    "ali_pred = pd.merge(ali_pred,outfit_full, on=['user_id','outfit_id'],how='left')\n",
    "ali_pred = ali_pred.drop('item_id_x',axis=1).rename(columns={'item_id_y':'item_id'})\n",
    "\n",
    "identical = pred_outfit.reset_index(drop=True).set_index(['user_id','item_id','outfit_id'])\n",
    "ali_pred = ali_pred.reset_index(drop=True).set_index(['user_id','item_id','outfit_id'])\n",
    "ali_pred = ali_pred[~ali_pred.index.isin(list(identical.index))]\n",
    "ali_pred = ali_pred.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_full = ali_val.copy()\n",
    "top5_ppl = val_full.item_id.value_counts().to_frame().index.astype('str')[:30]\n",
    "\n",
    "top5_ppl_train = ali_pred.drop_duplicates(['user_id'], keep='last').drop('item_id', axis=1)\n",
    "top5_ppl_train['item_id'] = [list(top5_ppl) for _ in range(len(top5_ppl_train))]\n",
    "top5_ppl_train = top5_ppl_train.explode('item_id')\n",
    "\n",
    "ali_pred = pd.concat([ali_pred, top5_ppl_train])\n",
    "ali_pred = ali_pred[~ali_pred.item_id.isna()]\n",
    "ali_pred = ali_pred.drop(['outfit_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = group_a.merge(img_feat, on='item_id',how='left')\n",
    "ali_pred = ali_pred.merge(img_feat,on='item_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_test, labels_test = make_test_pairs(group_a, ali_pred)\n",
    "test_1 = pairs_test[:, 0]\n",
    "test_2 = pairs_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c021256",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 5000\n",
    "for bucket in tqdm.tqdm(range(0, len(test_1), batch_size)):\n",
    "  outputs = siamese.predict(\n",
    "      [test_1[bucket: bucket+batch_size], test_2[bucket: bucket+batch_size] ]\n",
    "      )\n",
    "  predictions.append(outputs)\n",
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred = ali_pred.copy()\n",
    "sia_pred['pred'] = predictions\n",
    "sia_pred = sia_pred.sort_values(['user_id','pred'],ascending=False)\n",
    "sia_pred = sia_pred.groupby(['user_id','item_id']).img_embedding.agg('count').reset_index().sort_values('img_embedding',ascending=False)\n",
    "sia_pred = sia_pred.groupby('user_id').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5455c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred_lst = (\n",
    "    sia_pred\n",
    "    .groupby('user_id')[['item_id']]\n",
    "    .aggregate(lambda x: x.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(sia_pred_lst.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(calculate_recall_at_k(true, prediction, 5),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 10),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(map_at_k(true, prediction, 5),5))\n",
    "print(round(map_at_k(true, prediction, 10),5))\n",
    "print(round(map_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(calculate_ndcg_at_k(true, prediction,5),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,10),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,20),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06610a2f",
   "metadata": {},
   "source": [
    "# Contradictory-Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_user = pd.read_pickle('ali_user.pkl')\n",
    "ali_user = ali_user[ali_user.item_id.isin(list(img_feat.item_id))]\n",
    "outfit = ali_user.groupby('outfit_id').item_id.agg('count').reset_index().sort_values(['item_id'], ascending=False)\n",
    "outfit = outfit[outfit.item_id >=2]\n",
    "ali_user = ali_user[ali_user.outfit_id.isin(list(outfit.outfit_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_item = pd.read_csv('path/item_data.txt', header=None, delimiter=',',on_bad_lines='skip',\n",
    "                      names=['item_id', 'cate_id', 'imgLink', 'title'])\n",
    "ali_cate = ali_item[ali_item.item_id.isin(list(img_feat.item_id))]\n",
    "ali_cate = ali_cate.drop_duplicates('item_id')\n",
    "cate = ali_cate.groupby('cate_id').filter(lambda x: x['cate_id'].count()>2)\n",
    "ali_user = ali_user.merge(cate, on='item_id',how='left').reset_index(drop=True)\n",
    "ali_user = ali_user.drop(['imgLink','title'],axis=1)\n",
    "ali_user = ali_user.sample(frac = 1) # shuffle rows\n",
    "ali_train, ali_test, ali_val = np.split(ali_user, [int(0.5*len(ali_user)), int(0.6*len(ali_user))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ef02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = ali_val.drop_duplicates(subset=['user_id','outfit_id','item_id',])\n",
    "identical = pd.merge(ali_train.reset_index(), df_val, on=['user_id','outfit_id','item_id',],how='inner').set_index('index')\n",
    "df_val = ali_val.drop_duplicates(subset=['user_id','outfit_id'])\n",
    "neg = pd.merge(ali_train.reset_index(), df_val, on=['user_id','outfit_id'],how='inner').set_index('index')\n",
    "neg = neg[~neg.index.isin(list(identical.index))].reset_index(drop=True)\n",
    "neg = neg.rename(columns={'item_id_x':'item_id'}).drop('item_id_y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ali_train.reset_index(drop=True).set_index(['user_id','item_id'])\n",
    "neg = neg.reset_index(drop=True).set_index(['user_id','item_id'])\n",
    "pos = pos[~pos.index.isin(list(neg.index))].reset_index()\n",
    "neg = neg.reset_index()\n",
    "pos = pos[pos.user_id.isin(list(set(neg.user_id)))]\n",
    "pos = pos.groupby('user_id').filter(lambda x: len(x)>=7)\n",
    "neg = neg[['user_id','item_id','outfit_id','cate_id_x']].rename(columns={'cate_id_x':'cate_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cate = pd.merge(neg.reset_index(), df_val, on=['user_id','cate_id'],how='inner').set_index('index')\n",
    "neg_cate = neg_cate[~neg_cate.index.isin(list(identical.index))].reset_index(drop=True)\n",
    "neg_cate = neg_cate.rename(columns={'item_id_x':'item_id'}).drop('item_id_y',axis=1)\n",
    "neg_cate = neg_cate[['user_id','outfit_id_x','item_id','cate_id']].rename(columns={'outfit_id_x':'outfit_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cate = neg_cate.reset_index(drop=True).set_index(['user_id','item_id'])\n",
    "pos_cate = pos[~pos.index.isin(list(neg_cate.index))].reset_index()\n",
    "neg_cate = neg_cate.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e78ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cate = neg_cate.drop_duplicates(['user_id','item_id'])\n",
    "pos_cate = pos_cate.drop_duplicates(['user_id','item_id'])\n",
    "neg_cate = neg_cate[neg_cate.user_id.isin(list(set(pos_cate.user_id)))]\n",
    "pos_cate = pos_cate[pos_cate.user_id.isin(list(set(neg_cate.user_id)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = pos_cate.groupby('user_id').head(10)\n",
    "positives['label'] = 1\n",
    "negatives = neg_cate.groupby('user_id').head(10)\n",
    "negatives['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad40ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_full = pd.concat([positives, negatives],ignore_index=True)\n",
    "sia_full = sia_full.drop_duplicates(['user_id','item_id'])\n",
    "sia_full = pd.merge(sia_full, img_feat, on='item_id',how='left')\n",
    "sia_full = sia_full.sample(frac=1)\n",
    "sia_train, sia_val = np.split(sia_full, [int(0.8*len(sia_full))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, labels_train = make_pairs(sia_train)\n",
    "pairs_val, labels_val = make_pairs(sia_val)\n",
    "train_1 = pairs_train[:, 0] \n",
    "train_2 = pairs_train[:, 1]\n",
    "val_1 = pairs_val[:, 0]\n",
    "val_2 = pairs_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese.fit(\n",
    "    [train_1, train_2],\n",
    "    labels_train,\n",
    "    validation_data=([val_1, val_2], labels_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d37df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_pred = ali_test[['user_id']].drop_duplicates(subset='user_id')\n",
    "ali_pred = ali_pred.merge(pos, on='user_id',how='left')\n",
    "ali_pred = ali_pred.drop_duplicates(['user_id','item_id'])\n",
    "ali_pred = ali_pred.drop(['imgLink','title'],axis=1)\n",
    "ali_pred = ali_pred[~ali_pred.item_id.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_ppl = ali_val.groupby(['cate_id','item_id']).user_id.agg('count').reset_index().sort_values(\n",
    "                                    ['user_id'], ascending=False)\n",
    "top_cate_ppl = cate_ppl.groupby('cate_id')[['item_id']].aggregate(lambda x: x.tolist()).reset_index()\n",
    "\n",
    "for i in range(len(top_cate_ppl)):\n",
    "    top_cate_ppl.item_id[i] = top_cate_ppl.item_id[i][:10]\n",
    "    \n",
    "top_prod_pred = ali_pred[['user_id']].drop_duplicates(['user_id'], keep='last')\n",
    "\n",
    "top5_cate_ppl = top_cate_ppl.iloc[:10,:]\n",
    "df1_repeated = pd.concat([top5_cate_ppl] * len(top_prod_pred), ignore_index=True)\n",
    "df2_repeated = pd.concat([top_prod_pred] * len(top5_cate_ppl), ignore_index=True)\n",
    "df_combined = pd.concat([df1_repeated, df2_repeated], axis=1)\n",
    "\n",
    "cate_ppl = df_combined.explode('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_clicked = ali_val.drop_duplicates(['user_id','cate_id'])\n",
    "cate_clicked = cate_clicked[cate_clicked.user_id.isin(list(set(ali_pred.user_id)))].reset_index().set_index(['user_id','cate_id'])\n",
    "cate_ppl = cate_ppl.reset_index(drop=True).set_index(['user_id','cate_id'])\n",
    "cate_ppl = cate_ppl[~cate_ppl.index.isin(list(cate_clicked.index))]\n",
    "cate_ppl = cate_ppl.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfit_full = ali_user.drop_duplicates(subset='outfit_id')\n",
    "outfit_full = outfit_full[['outfit_id','item_id']]\n",
    "cate_ppl = pd.merge(cate_ppl,outfit_full,on='item_id',how='left')\n",
    "ali_pred = pd.concat([ali_pred, cate_ppl],ignore_index=True)\n",
    "\n",
    "ali_pred = ali_pred.groupby('user_id').head(85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a893ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = ali_test[['user_id']].drop_duplicates(subset='user_id')\n",
    "click = pos.groupby('user_id').head(1)\n",
    "group_a = group_a.merge(click, on='user_id',how='left')\n",
    "ppl = ali_train.item_id.value_counts().to_frame().index.astype('str')[0]\n",
    "group_a['item_id'] = group_a['item_id'].fillna(ppl)\n",
    "group_a = group_a[group_a.user_id.isin(list(set(ali_pred.user_id)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = group_a.merge(img_feat, on='item_id',how='left')\n",
    "ali_pred = ali_pred.merge(img_feat,on='item_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e98978",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_test, labels_test = make_test_pairs(group_a, ali_pred)\n",
    "test_1 = pairs_test[:, 0]\n",
    "test_2 = pairs_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 5000\n",
    "for bucket in tqdm.tqdm(range(0, len(test_1), batch_size)):\n",
    "  outputs = siamese.predict(\n",
    "      [test_1[bucket: bucket+batch_size], test_2[bucket: bucket+batch_size] ]\n",
    "      )\n",
    "  predictions.append(outputs)\n",
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred = ali_pred.copy()\n",
    "sia_pred['pred'] = predictions\n",
    "sia_pred = sia_pred.sort_values(['user_id','pred'],ascending=False)\n",
    "sia_pred = sia_pred.groupby(['user_id','item_id']).img_embedding.agg('count').reset_index().sort_values('img_embedding',ascending=False)\n",
    "sia_pred = sia_pred.groupby('user_id').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_pred_lst = (\n",
    "    sia_pred\n",
    "    .groupby('user_id')[['item_id']]\n",
    "    .aggregate(lambda x: x.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c82352",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(sia_pred_lst.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eecf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(calculate_recall_at_k(true, prediction, 5),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 10),5))\n",
    "print(round(calculate_recall_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(map_at_k(true, prediction, 5),5))\n",
    "print(round(map_at_k(true, prediction, 10),5))\n",
    "print(round(map_at_k(true, prediction, 20),5))\n",
    "\n",
    "print(round(calculate_ndcg_at_k(true, prediction,5),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,10),5))\n",
    "print(round(calculate_ndcg_at_k(true, prediction,20),5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
